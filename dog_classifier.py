# -*- coding: utf-8 -*-
"""Untitled29.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10KKBfNObCiGTMZKJalZJBHOatUddhP0S

## Dog Breed Classifier

## Imports
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torchvision
from torchvision.transforms import v2
import torch
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as f
from PIL import Image
import requests
import tqdm
from google.colab import drive

"""## Getting the Data"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
transforms = v2.Compose([
    v2.Resize(256),
    v2.CenterCrop(212),
    v2.ToTensor(),
    v2.RandomHorizontalFlip(p = 0.5),
    v2.ColorJitter(0.2,0.2,0.2,0.1)
])
drive.mount('/content/drive')
trainset = torchvision.datasets.ImageFolder('/content/drive/MyDrive/Dog/train/', transform = transforms)
validset = torchvision.datasets.ImageFolder('/content/drive/MyDrive/Dog/valid/', transform = transforms)
testset = torchvision.datasets.ImageFolder('/content/drive/MyDrive/Dog/test/', transform = transforms)


batch_size = 128


trainloader = torch.utils.data.DataLoader(trainset , batch_size=batch_size , shuffle = True)
validloader = torch.utils.data.DataLoader(validset , batch_size=batch_size , shuffle = True)
testloader = torch.utils.data.DataLoader(testset  , batch_size=batch_size)

def imshow(img):
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()

# Get the first batch of training data
dataiter = iter(trainloader)
images, labels = next(dataiter)

# Display the first image in the batch
imshow(torchvision.utils.make_grid(images[0]))

images, labels = images.numpy() , labels.numpy()

fig = plt.figure(figsize = (20,10))

for i in range(0,16):
    toAdd = fig.add_subplot(2 , int(batch_size/16) , i + 1)
    toAdd.imshow(np.transpose(images[i] , (1,2,0)))
    toAdd.set_title(trainset.classes[labels[i]])

"""## Training The Model"""

model = torchvision.models.resnet18(pretrained=True)
model.fc = nn.Sequential(
                      nn.Linear(model.fc.in_features, 256),
                      nn.ReLU(),
                      nn.Dropout(0.2),
                      nn.Conv2d(256, 196, kernel_size=[10,10]),
                      nn.Linear(196, len(trainset.classes)),
                      nn.LogSoftmax(dim=1))


model = model.to(device)
for inputs, labels in trainloader:
    inputs, labels = inputs.to(device), labels.to(device)

for inputs, labels in validloader:
    inputs, labels = inputs.to(device), labels.to(device)

for inputs, labels in testloader:
    inputs, labels = inputs.to(device), labels.to(device)
def init_weights(model):
    for m in model.modules():
        if isinstance(m, (nn.Conv2d, nn.Linear)):
            nn.init.normal_(m.weight, mean=0, std=0.005)
# still determining whether or not a pre-trained model is what I should use
def fit_one_epoch(model, opt, loader):
    losses, accuracies = [], []
    for images, labels in tqdm.tqdm(loader):
        prediction = model(images)
        l = loss(prediction, labels)
        acc = (prediction.argmax(1) == labels).float().mean()

        l.backward()
        opt.step()
        opt.zero_grad()

        losses.append(l.detach().item())
        accuracies.append(acc.detach().item())
    return np.mean(losses), np.mean(accuracies)

@torch.no_grad()
def eval(model, loader):
    accuracies = []
    for images, labels in tqdm.tqdm(loader):
        prediction = model(images)
        acc = (prediction.argmax(1) == labels).float().mean()
        accuracies.append(acc.detach().item())
    return np.mean(accuracies)


def fit(model, loader_train, loader_test, epochs=10):
    trl_min =1000
    num_epochs_without_improvement= 0
    opt = torch.optim.Adam(model.parameters(), lr=0.0005)
    hist_tr_loss, hist_tr_acc, hist_te_acc = [], [], []
    for epoch in range(epochs):
        tr_l, tr_acc = fit_one_epoch(model, opt, loader_train)
        if(tr_l < trl_min):
            num_epochs_without_improvement+=1
            if(num_epochs_without_improvement>=5):
              print("The model is no longer improving")
              return hist_tr_loss, hist_tr_acc, hist_te_acc

        te_acc = eval(model, loader_test)

        print(f"Finished epoch {epoch} of {epochs}: Train Loss = {tr_l:.3f}   Train Acc = {tr_acc:.3f}   Test Acc = {te_acc:.3f}", flush=True)
        hist_tr_loss.append(tr_l)
        hist_tr_acc.append(tr_acc)
        hist_te_acc.append(te_acc)
    return hist_tr_loss, hist_tr_acc, hist_te_acc

loss = nn.CrossEntropyLoss()
#init_weights(model)
hist_tr_loss, hist_tr_acc, hist_te_acc = fit(model, trainloader, validloader, epochs=30)
print("Our mean accuracy on the testloader is", eval(model, testloader))